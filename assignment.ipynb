{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e9977db",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import wordninja\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from gensim.corpora import Dictionary\n",
    "from gensim.models import LdaModel\n",
    "from pprint import pprint\n",
    "import os\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99d8f474",
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82fa0e54",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_statuses(fname):\n",
    "\n",
    "    pattern = re.compile(\n",
    "    r'_H.R.\\d{1,4}_(?P<status>.+?) \\((?P<date>\\d{2}_\\d{2}_\\d{4})\\)\\.txt$'\n",
    "    )\n",
    "\n",
    "    m = pattern.search(fname)\n",
    "    if m:\n",
    "        return m.group('status').strip(), m.group('date').strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad815ca9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_boilerplate(text: str) -> str:\n",
    "    pattern =  re.compile(r'^Shown Here:.*?\\((\\d{2}/\\d{2}/\\d{4})\\)')        # captures date MM/DD/YYYY\n",
    "    text = re.sub(pattern, '', text)                                        # removes boilerplate text\n",
    "    parenthesis_content_pattern = re.compile(r'\\([^)]*\\)')\n",
    "    text = re.sub(parenthesis_content_pattern, '', text)                                 # captures content in parenthesis\n",
    "    return text if text else None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "621c907b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_divisions_heading(text: str) -> str:\n",
    "\n",
    "    header_re = re.compile(\n",
    "        r'DIVISION\\s+'      # the word DIVISION and some space\n",
    "        r'[A-Z]--'          # one uppercase letter, two hyphens\n",
    "        r'[A-Z\\s,]+'        # all‑caps title (letters, spaces, commas)\n",
    "    )\n",
    "    cleaned_text = re.sub(header_re, '', text)\n",
    "    return cleaned_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa3f49b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stopwords(text : str) -> str:\n",
    "\n",
    "    stops = set(stopwords.words('english'))\n",
    "    no_divisions_headers = remove_divisions_heading(text)\n",
    "    tokens = word_tokenize(no_divisions_headers)\n",
    "    words = [w.lower() for w in tokens if w.lower() not in stops and w.isalpha()]\n",
    "    return words if words else None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d101aaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "folder_path = 'summaries118'\n",
    "data = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f114c34b",
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx,entry in enumerate(os.scandir(folder_path)):\n",
    "\n",
    "    if not entry.is_file():\n",
    "        continue\n",
    "\n",
    "    file_path = os.path.join(folder_path, entry.name)\n",
    "    with open (file_path, 'r', encoding='utf-8') as f:\n",
    "        corpus = f.read()\n",
    "    status,date = get_statuses(entry.name)\n",
    "    rec = pd.DataFrame({\n",
    "        'status': status,\n",
    "        'date': date.replace('_','/'),\n",
    "        'content': corpus\n",
    "    }, index=[idx])\n",
    "    data = pd.concat([data,rec], ignore_index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59660901",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(content: str) -> str:\n",
    "\n",
    "    no_boilerplate = remove_boilerplate(content)\n",
    "\n",
    "    if not no_boilerplate:\n",
    "        return None\n",
    "    \n",
    "    no_stops = remove_stopwords(no_boilerplate)\n",
    "\n",
    "    return no_stops if no_stops else None "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0ef3ae1",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['cleaned_content'] = data['content'].map(lambda x: clean_text(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5eca4e11",
   "metadata": {},
   "source": [
    "# **1. Topic Modeling**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a74bfcc4",
   "metadata": {},
   "source": [
    "## **A. LDA**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e48d27fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary = Dictionary(data['cleaned_content'].tolist())\n",
    "dictionary.filter_extremes(no_below=0.01, no_above=0.25)\n",
    "corpus = [dictionary.doc2bow(text) for text in data['cleaned_content'].tolist() if text]\n",
    "\n",
    "temp = dictionary[0]\n",
    "\n",
    "id2word = dictionary.id2token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c01851ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LdaModel(\n",
    "    corpus=corpus,\n",
    "    id2word=id2word,\n",
    "    num_topics=10,\n",
    "    iterations=1000,\n",
    "    random_state=42,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03dc508e",
   "metadata": {},
   "outputs": [],
   "source": [
    "topics = model.print_topics(num_words=10)\n",
    "topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38cda483",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(topics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42416330",
   "metadata": {},
   "outputs": [],
   "source": [
    "topics[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07747056",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyLDAvis\n",
    "import pyLDAvis.gensim_models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4bf59d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "pyLDAvis.enable_notebook()\n",
    "vis = pyLDAvis.gensim_models.prepare(model, corpus, dictionary)\n",
    "pyLDAvis.display(vis)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2903c436",
   "metadata": {},
   "source": [
    "##  **B. BERTtopic**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "184a3544",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bertopic import BERTopic\n",
    "from sentence_transformers import SentenceTransformer, losses, InputExample\n",
    "from torch.utils.data import DataLoader\n",
    "from umap import UMAP\n",
    "from hdbscan import HDBSCAN\n",
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e67d7f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "\n",
    "examples = [ InputExample(texts=[doc_a, doc_b], label=0.8) for doc_a, doc_b in zip(data['content'].tolist(), data['content'][1:].tolist()) ]\n",
    "loader   = DataLoader(examples, batch_size=16, shuffle=True)\n",
    "\n",
    "# 3) Choose a loss (e.g. cosine similarity)\n",
    "train_loss = losses.CosineSimilarityLoss(model)\n",
    "\n",
    "# 4) Fine‑tune\n",
    "embedding_model.fit(train_objectives=[(loader, train_loss)], epochs=2, warmup_steps=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd4be6ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer_model = CountVectorizer(\n",
    "  ngram_range=(1,2),    \n",
    "  stop_words=\"english\",\n",
    "  min_df=5,             \n",
    "  max_df=0.75            \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c555331",
   "metadata": {},
   "outputs": [],
   "source": [
    "umap_model = UMAP(\n",
    "    n_neighbors=30, \n",
    "    n_components=10, \n",
    "    min_dist=0.1,\n",
    "    metric='cosine', \n",
    "    random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "714ec5cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "hdbscan_model = HDBSCAN(\n",
    "    min_cluster_size=20,        \n",
    "    min_samples=5,              \n",
    "    cluster_selection_epsilon=0.0, \n",
    "    cluster_selection_method='eom',  \n",
    "    metric='euclidean',        \n",
    "    prediction_data=True        \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "532d5332",
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_model = BERTopic(embedding_model=embedding_model,\n",
    "                       umap_model=umap_model,      \n",
    "                       hdbscan_model=hdbscan_model, \n",
    "                       calculate_probabilities=True,\n",
    "                       vectorizer_model=vectorizer_model,  \n",
    "                       verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "713eb917",
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = data['content'].tolist()\n",
    "topics, probs = topic_model.fit_transform(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15619ff7",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(topics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12f7a818",
   "metadata": {},
   "outputs": [],
   "source": [
    "topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68e64192",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = topic_model.visualize_topics()\n",
    "fig.write_html(\"./topics.html\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24f9eddf",
   "metadata": {},
   "source": [
    "# **2. Semantic Similarity**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e881e65c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp_a",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
